# integration_test.py
#
# Purpose:
# This script tests the logging and differentiation of "real" user sessions
# versus "simulated" user sessions (generated by simulate_user_session.py).
# It verifies that both types of sessions are logged correctly to the
# chatbot_sessions.sqlite database with appropriate 'source' tags and other metadata.
#
# Setup and Dependencies:
# 1. Python 3 installed.
# 2. 'chatbot_sessions.sqlite' database file must exist and be accessible.
#    Update DB_FILE if it's located elsewhere.
# 3. 'scripts/simulate_user_session.py' must be executable and correctly configured
#    to log to the SAME 'chatbot_sessions.sqlite' database.
# 4. 'scripted_inputs.jsonl' must exist in the project root or the path expected
#    by 'simulate_user_session.py' for the simulation to run.
# 5. Placeholder functions for "real" interaction and logging (`log_real_interaction`
#    and `actual_log_message_to_db`) MUST be replaced with your application's
#    actual functions for handling user queries and database logging.
#
# How to Adapt:
# - DB_FILE: Path to your SQLite database.
# - SIMULATE_SCRIPT_PATH: Path to your simulate_user_session.py.
# - COLUMN_MAPPING: Adjust if your database table/column names differ.
# - `actual_log_message_to_db`: Replace this placeholder with the import and call
#   to your project's actual database logging function (e.g., from src.logging_utils).
#   Ensure its signature matches the way it's called in `log_real_interaction`.
# - `get_actual_chatbot_response`: Replace this with a call to your chatbot's
#   core logic to get a response for the "real" interaction part.
# - REAL_USER_SOURCE_TAG: Define what 'source' tag your actual application uses for
#   non-simulated user interactions. This test will check that real entries have this tag
#   or are distinct from 'simulated'/'simulated_feedback'.

import sqlite3
import subprocess
import os
import uuid
from datetime import datetime, timezone
import json
import time

# --- Configuration ---
DB_FILE = "chatbot_sessions.sqlite"
SIMULATE_SCRIPT_PATH = os.path.join("scripts", "simulate_user_session.py")

# Expected source tag for "real" user interactions.
# This could be 'user', 'streamlit_app', None, or any tag your app uses.
# The test will verify that real entries are NOT 'simulated' or 'simulated_feedback'.
# For a more precise check, set this to the exact tag your app uses.
REAL_USER_SOURCE_TAG = "real_user_interaction" # Example, adjust as needed

# Column names mapping (conceptual_field_name: actual_db_column_name)
COLUMN_MAPPING = {
    "session_id": "session_id",
    "user_prompt": "user_message",
    "bot_response": "bot_message",
    "timestamp": "timestamp",
    "source": "source",
    "module": "module",
    "rating": "rating",
    # "feedback_text_col": "bot_message", # if feedback text is in bot_message for feedback entries
    # "feedback_type_col": "type"       # if feedback has a 'type' column
}
# Fields considered essential for the script's core functionality
REQUIRED_CONCEPTUAL_FIELDS = ["session_id", "timestamp", "source"]


# --- Placeholder for Actual Application Logic ---
# YOU MUST REPLACE THESE WITH YOUR ACTUAL APPLICATION'S FUNCTIONS

def get_actual_chatbot_response(user_input: str, session_id: str) -> str:
    """
    Placeholder for your application's core chatbot response generation.
    Replace this with: from your_module import get_response_function
    """
    print(f"INTEGRATION_TEST_PLACEHOLDER: Getting real chatbot response for: '{user_input}'")
    return f"Actual bot response to: {user_input}"

def actual_log_message_to_db(session_id: str, user_message: str, bot_message: str, metadata: dict):
    """
    Placeholder for your application's database logging function.
    Replace this with: from src.logging_utils import log_message_to_db
    This function should write to the DB_FILE.
    """
    print(f"INTEGRATION_TEST_PLACEHOLDER_LOG: Logging to DB (session: {session_id}):")
    print(f"  User: {user_message}")
    print(f"  Bot: {bot_message}")
    print(f"  Metadata: {metadata}")

    # Example of how it might actually write to the database:
    # conn = sqlite3.connect(DB_FILE)
    # cursor = conn.cursor()
    # try:
    #     cursor.execute(
    #         f"INSERT INTO {COLUMN_MAPPING.get('messages_table', 'messages')} "
    #         f"({COLUMN_MAPPING['session_id']}, {COLUMN_MAPPING['user_prompt']}, {COLUMN_MAPPING['bot_response']}, "
    #         f"{COLUMN_MAPPING['timestamp']}, {COLUMN_MAPPING['source']}, {COLUMN_MAPPING['module']}) "
    #         "VALUES (?, ?, ?, ?, ?, ?)",
    #         (session_id, user_message, bot_message, metadata.get('timestamp'),
    #          metadata.get('source'), metadata.get('module'))
    #     )
    #     conn.commit()
    # except sqlite3.Error as e:
    #     print(f"INTEGRATION_TEST_PLACEHOLDER_LOG: DB Error: {e}")
    # finally:
    #     conn.close()
    pass # Remove pass when implementing

# --- Test Helper Functions ---

def log_real_interaction(test_session_id: str, user_input: str, module_name: str = "TestModule"):
    """Simulates logging a 'real' user interaction."""
    print(f"\nINTEGRATION_TEST: Simulating real user interaction for session {test_session_id}...")
    bot_response = get_actual_chatbot_response(user_input, test_session_id)
    
    timestamp_now = datetime.now(timezone.utc).isoformat()
    log_metadata = {
        "timestamp": timestamp_now,
        "source": REAL_USER_SOURCE_TAG, # Crucial for differentiation
        "module": module_name,
        # Add any other metadata your real logger includes
    }
    
    # Call the (placeholder) actual logging function
    actual_log_message_to_db(
        session_id=test_session_id,
        user_message=user_input,
        bot_message=bot_response,
        metadata=log_metadata
    )
    print(f"INTEGRATION_TEST: 'Real' interaction logged for session {test_session_id}.")
    return test_session_id, timestamp_now

def run_simulation_script():
    """Runs the simulate_user_session.py script."""
    print(f"\nINTEGRATION_TEST: Running '{SIMULATE_SCRIPT_PATH}'...")
    try:
        # Ensure the script is executable or called via python interpreter
        process = subprocess.run(['python', SIMULATE_SCRIPT_PATH], capture_output=True, text=True, check=True, timeout=60)
        print(f"INTEGRATION_TEST: '{SIMULATE_SCRIPT_PATH}' ran successfully.")
        print("Output from simulation script:")
        print(process.stdout)
        if process.stderr:
            print("Stderr from simulation script:")
            print(process.stderr)
        return True
    except subprocess.CalledProcessError as e:
        print(f"INTEGRATION_TEST_ERROR: '{SIMULATE_SCRIPT_PATH}' failed.")
        print(f"Return code: {e.returncode}")
        print(f"Stdout: {e.stdout}")
        print(f"Stderr: {e.stderr}")
        return False
    except subprocess.TimeoutExpired:
        print(f"INTEGRATION_TEST_ERROR: '{SIMULATE_SCRIPT_PATH}' timed out.")
        return False
    except FileNotFoundError:
        print(f"INTEGRATION_TEST_ERROR: Simulation script '{SIMULATE_SCRIPT_PATH}' not found.")
        return False

def get_db_connection():
    if not os.path.exists(DB_FILE):
        print(f"INTEGRATION_TEST_ERROR: Database file '{DB_FILE}' not found.")
        return None
    try:
        conn = sqlite3.connect(DB_FILE)
        conn.row_factory = sqlite3.Row
        return conn
    except sqlite3.Error as e:
        print(f"INTEGRATION_TEST_ERROR: Could not connect to database '{DB_FILE}': {e}")
        return None

def get_table_columns(conn, table_name):
    cursor = conn.cursor()
    cursor.execute(f"PRAGMA table_info('{table_name}');")
    return [info[1] for info in cursor.fetchall()]

def verify_logs(conn, test_start_time_iso):
    """Queries the database and verifies the logged entries."""
    print("\nINTEGRATION_TEST: Verifying database logs...")
    
    # Attempt to guess messages table if not explicitly defined in COLUMN_MAPPING
    messages_table = COLUMN_MAPPING.get("messages_table", "messages") # Default to 'messages'
    
    actual_db_columns = get_table_columns(conn, messages_table)
    if not actual_db_columns:
        print(f"  FAIL: Could not get columns for table '{messages_table}'. Test aborted.")
        return {"overall": "FAIL", "details": ["DB table/column fetch error"]}

    # Construct SELECT clauses based on available columns and mapping
    select_clauses = []
    valid_conceptual_fields = []
    for conceptual_name, db_col_name in COLUMN_MAPPING.items():
        if db_col_name in actual_db_columns:
            select_clauses.append(f'"{db_col_name}" AS "{conceptual_name}"')
            valid_conceptual_fields.append(conceptual_name)
        elif conceptual_name in REQUIRED_CONCEPTUAL_FIELDS:
            print(f"  FAIL: Required column '{db_col_name}' (for '{conceptual_name}') not found in '{messages_table}'.")
            return {"overall": "FAIL", "details": [f"Missing required DB column: {db_col_name}"]}

    if not COLUMN_MAPPING["timestamp"] in actual_db_columns or not COLUMN_MAPPING["source"] in actual_db_columns:
        print(f"  FAIL: Essential columns for timestamp ('{COLUMN_MAPPING['timestamp']}') or source ('{COLUMN_MAPPING['source']}') not found in '{messages_table}'.")
        return {"overall": "FAIL", "details": ["Missing timestamp/source DB columns"]}

    query = f"""
        SELECT {', '.join(select_clauses)}
        FROM "{messages_table}"
        WHERE "{COLUMN_MAPPING['timestamp']}" >= ?
        ORDER BY "{COLUMN_MAPPING['timestamp']}" ASC
    """
    
    cursor = conn.cursor()
    try:
        cursor.execute(query, (test_start_time_iso,))
        rows = cursor.fetchall()
    except sqlite3.Error as e:
        print(f"  FAIL: Database query error: {e}")
        return {"overall": "FAIL", "details": [f"DB query error: {e}"]}

    if not rows:
        print("  FAIL: No new log entries found after test start time.")
        return {"overall": "FAIL", "details": ["No new logs found"]}

    print(f"  Found {len(rows)} new log entries since test started.")

    checks = {
        "real_session_found": False,
        "simulated_session_found": False,
        "simulated_feedback_found": False, # Optional, but good to check if simulation script creates it
        "real_source_correct": True,
        "simulated_source_correct": True,
        "simulated_feedback_source_correct": True,
        "all_have_session_id": True,
        "all_have_timestamp": True,
        "feedback_association_correct": True # Placeholder for more specific feedback checks
    }
    
    real_session_ids = set()
    simulated_session_ids = set()

    for row in rows:
        session_id = row["session_id"] if "session_id" in valid_conceptual_fields else None
        timestamp = row["timestamp"] if "timestamp" in valid_conceptual_fields else None
        source = row["source"] if "source" in valid_conceptual_fields else None

        if not session_id: checks["all_have_session_id"] = False
        if not timestamp: checks["all_have_timestamp"] = False

        if source == REAL_USER_SOURCE_TAG:
            checks["real_session_found"] = True
            real_session_ids.add(session_id)
        elif source == "simulated":
            checks["simulated_session_found"] = True
            simulated_session_ids.add(session_id)
            if not (row["module"] if "module" in valid_conceptual_fields else None): # Simulated entries should have module
                print(f"  WARN: Simulated entry for session {session_id} missing module.")
        elif source == "simulated_feedback":
            checks["simulated_feedback_found"] = True
            simulated_session_ids.add(session_id) # Feedback belongs to a simulated session
            # Further checks for feedback: e.g., rating, feedback text
            if not (row["rating"] if "rating" in valid_conceptual_fields else None) and not (row["bot_response"] if "bot_response" in valid_conceptual_fields else None):
                 print(f"  WARN: Simulated feedback entry for session {session_id} missing rating or text.")
        elif source in ["simulated", "simulated_feedback"]: # Should not happen if caught by above
            checks["simulated_source_correct"] = False # Should be one of the specific tags
        else: # Should be a real session if not simulated
            checks["real_session_found"] = True # Assume any other non-null source is real for this test
            real_session_ids.add(session_id)
            if source is None or source == "":
                 print(f"  INFO: Real session entry for {session_id} has blank/None source. This might be expected.")
            # If REAL_USER_SOURCE_TAG is specific, this else implies a mismatch for real source
            # For now, we are more lenient: if not 'simulated*', it's 'real' for presence check.

    if not checks["real_session_found"]: print("  FAIL: No 'real' session logs found.")
    if not checks["simulated_session_found"]: print("  FAIL: No 'simulated' session logs found.")
    # If simulate_user_session.py is expected to always create feedback:
    # if not checks["simulated_feedback_found"]: print("  WARN: No 'simulated_feedback' logs found (might be ok if simulation doesn't always create it).")

    if not checks["all_have_session_id"]: print("  FAIL: Not all entries have a session_id.")
    if not checks["all_have_timestamp"]: print("  FAIL: Not all entries have a timestamp.")

    # Check if feedback (if any) is associated with a known simulated session
    if checks["simulated_feedback_found"] and not simulated_session_ids:
        checks["feedback_association_correct"] = False
        print("  FAIL: Simulated feedback found, but no parent simulated session ID captured.")
    
    # Determine overall pass/fail
    overall_status = "PASS"
    fail_details = []
    if not checks["real_session_found"]: fail_details.append("Real session missing")
    if not checks["simulated_session_found"]: fail_details.append("Simulated session missing")
    if not checks["all_have_session_id"]: fail_details.append("Session ID missing in some logs")
    if not checks["all_have_timestamp"]: fail_details.append("Timestamp missing in some logs")
    # Add more critical checks as needed

    if fail_details:
        overall_status = "FAIL"
        
    summary = {"overall": overall_status, "details": checks, "fail_messages": fail_details}
    return summary

def main():
    print("--- Starting Integration Test for Session Logging ---")
    
    # Record start time to filter logs
    # Give a little buffer before to catch entries logged right at the start
    test_start_time = datetime.now(timezone.utc) - timezone.timedelta(seconds=5)
    test_start_time_iso = test_start_time.isoformat()

    # 1. Simulate and log a "real" user interaction
    #    YOU MUST ENSURE `actual_log_message_to_db` correctly logs to your DB.
    real_session_id = "real_test_session_" + str(uuid.uuid4())
    log_real_interaction(real_session_id, "Hello from real user test!", "RealUserModule")
    
    # Small delay to ensure "real" log is written before simulation starts,
    # especially if logging is asynchronous in the actual application.
    time.sleep(2)

    # 2. Run the simulation script to log a simulated session
    simulation_successful = run_simulation_script()
    if not simulation_successful:
        print("Aborting test due to simulation script failure.")
        # Still try to check if the real log made it
        conn_check = get_db_connection()
        if conn_check:
            verify_logs(conn_check, test_start_time_iso) # Check what we have
            conn_check.close()
        return

    # 3. Connect to DB and verify logs
    conn = get_db_connection()
    if not conn:
        print("Aborting test due to DB connection failure.")
        return

    results = verify_logs(conn, test_start_time_iso)
    conn.close()

    # 4. Print summary
    print("\n--- Integration Test Summary ---")
    print(f"Overall Status: {results.get('overall', 'ERROR')}")
    if results.get('fail_messages'):
        print("Failures:")
        for msg in results['fail_messages']:
            print(f"  - {msg}")
            
    print("\nDetailed Checks:")
    for check, status in results.get('details', {}).items():
        print(f"  {check}: {'PASS' if status else 'FAIL' if isinstance(status, bool) else status}")
    
    if results.get('overall', 'ERROR') != "PASS":
        print("\nOne or more critical checks failed. Please review logs and script configurations.")
    else:
        print("\nAll critical checks passed!")

    print("--- Integration Test Finished ---")

if __name__ == "__main__":
    # This is crucial: the actual_log_message_to_db needs to be the *real* one from the project.
    # If it's still the placeholder, the "real" interaction won't actually be in the DB.
    # Example:
    # from src.logging_utils import log_message_to_db as actual_db_logger
    # actual_log_message_to_db = actual_db_logger # Monkey patch or assign
    
    # For this script to run as-is with placeholders, the placeholder actual_log_message_to_db
    # would need to be modified to actually write to the SQLite DB for the test to be meaningful.
    # As it stands, it only prints. The user of this script needs to integrate their true logger.
    main()