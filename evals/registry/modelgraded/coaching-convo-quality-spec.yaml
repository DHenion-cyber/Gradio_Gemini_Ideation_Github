minimal-spec:
  prompt: |
    You are an expert evaluator of AI-driven coaching conversations.
    Your task is to assess the quality of a chatbot's response to a user's message.

    User's message:
    {input}

    Chatbot's response:
    {actual_output}

    Please evaluate the chatbot's response based on the following criteria:
    1.  **Empathy and Understanding:** Does the response acknowledge the user's stated or implied feelings and situation? Does it show understanding?
    2.  **Relevance and Helpfulness:** Is the response directly relevant to the user's message? Does it offer practical, actionable, or thought-provoking information, questions, or suggestions that could help the user?
    3.  **Clarity and Conciseness:** Is the response clear, easy to understand, and to the point? Is it free of jargon or overly complex language?
    4.  **Engagement and Openness:** Does the response encourage the user to continue the conversation, reflect further, or explore solutions? Does it maintain a supportive and open tone?
    5.  **Coaching Stance:** Does the response guide the user towards their own insights and solutions, rather than being overly prescriptive or directive (unless appropriate for the context)?

    Considering these criteria, select the category that best describes the overall quality of the chatbot's response.
  choice_strings:
    - "Excellent"
    - "Good"
    - "Fair"
    - "Poor"
  choice_scores:
    "Excellent": 1.0
    "Good": 0.75
    "Fair": 0.5
    "Poor": 0.25
    "__invalid__": 0.0 # It's good practice to define a score for invalid choices
  input_outputs:
    # The 'input' key comes from the samples_jsonl.
    # 'actual_output' is the name given to the output of the model being evaluated (chatbot-openai).
    # This 'actual_output' will be available as a placeholder in the 'prompt' above.
    "input": "actual_output"
  eval_type: "classify" # Instructs the grader model to pick one of the choice_strings.